{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ALL_MODELS_CLASS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RachitBansal/AppliancePower_TimeSeries/blob/master/ALL_MODELS_CLASS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rU0SxZHFwyah",
        "outputId": "cdd8fa7a-70db-4557-bd11-1434b607cbdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "####code to  upload the data and resample the data (day wise)\n",
        "from sklearn.externals import joblib\n",
        "from keras.layers import Dense ,LSTM\n",
        "from keras.models import Sequential \n",
        "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Flatten\n",
        "from sklearn.metrics import r2_score as r2\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from pandas import DataFrame\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "# import r2\n",
        "# import mane\n",
        "\n",
        "import keras.backend as K\n",
        "\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vwHoyoEboN9L",
        "outputId": "3097cc8f-41aa-4567-d793-397b8f5aba9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PJUZ2WAuwyat",
        "colab": {}
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def r2_score(y_true, y_pred):\n",
        "    SS_res =  K.sum(K.square(y_true-y_pred))\n",
        "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
        "    return (1 - SS_res/(SS_tot + K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7Jpn9ZS5rWeU",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "import xgboost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O7arjjcje4sr",
        "colab": {}
      },
      "source": [
        "class Ensemble_pipeline():\n",
        "    def __init__(self, equipment, n_members):\n",
        "        self.eq = equipment\n",
        "        self.n_members = n_members\n",
        "\n",
        "    def load_arima_res(self):\n",
        "        return np.load('./drive/My Drive/UkDale/ARIMA/arima_ukdale_preds_1000_eq'+str(self.eq)+'.npy')\n",
        "\n",
        "    def fit_cnn_lstm(self,epochs=20,batch_size=128): \n",
        "        n_timesteps, n_features = self.train_x.shape[1], self.train_x.shape[2]\n",
        "        model = Sequential()\n",
        "        model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,1)))\n",
        "        model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Flatten())\n",
        "        model.add(RepeatVector(1))\n",
        "        model.add(LSTM(200, activation='relu', return_sequences=True))\n",
        "        model.add(TimeDistributed(Dense(100, activation='relu')))\n",
        "        model.add(TimeDistributed(Dense(1)))\n",
        "        # model.summary()\n",
        "        model.compile(loss='mse', optimizer='adam',metrics = [\"accuracy\"])\n",
        "\n",
        "        model.fit(self.train_x, self.train_y, epochs=10, batch_size=128, verbose = 1)\n",
        "\n",
        "        return model\n",
        "    \n",
        "    def fit_ANN(self,epochs=20,batch_size=128):\n",
        "        x_train = self.train_x.reshape(-1, 128)\n",
        "        ann = Sequential()\n",
        "        ann.add(Dense(256,activation = 'relu',input_dim = 128))\n",
        "        ann.add(Dense(64,activation = 'relu'))\n",
        "        ann.add(Dense(64,activation = 'relu'))\n",
        "        ann.add(Dense(1,activation = 'relu'))\n",
        "        \n",
        "        ann.compile (optimizer = 'RMSprop',loss = 'mean_squared_error',metrics = ['accuracy'])\n",
        "        ann.fit(x_train,self.train_y[:,i], batch_size = batch_size, epochs = epochs)\n",
        "        \n",
        "        return ann\n",
        "    \n",
        "    def fit_vanilla_lstm(self,epochs=20,batch_size=128):\n",
        "        regressor = Sequential()\n",
        "        regressor.add(LSTM(units = 10,activation = \"relu\",input_shape= (128,1)))\n",
        "        regressor.add(Dense(units = 1))\n",
        "        regressor.compile(optimizer = \"adam\",loss = \"mean_squared_error\")\n",
        "        regressor.fit(self.train_x,self.train_y,epochs = epochs, batch_size = batch_size,verbose  = 0)\n",
        " \n",
        "        return regressor\n",
        "   \n",
        "    def load_data(self):\n",
        "        self.train_x = np.load(file='./drive/My Drive/UkDale/ukdale_'+str(self.eq)+'_x.npy')\n",
        "        self.train_y = np.load(file='./drive/My Drive/UkDale/ukdale_'+str(self.eq)+'_y.npy')\n",
        "        test_x = np.load(file='./drive/My Drive/UkDale/ukdale_'+str(self.eq)+'_tx.npy')\n",
        "        test_y = np.load(file='./drive/My Drive/UkDale/ukdale_'+str(self.eq)+'_ty.npy')\n",
        "        val_split = int(0.2*test_x.shape[0])\n",
        "        self.val_x = test_x[-val_split:]\n",
        "        self.val_y = test_y[-val_split:]\n",
        "        self.test_x = test_x[:-val_split]\n",
        "        self.test_y = test_y[:-val_split]\n",
        "        print(self.val_x.shape, self.val_y.shape, self.test_x.shape, self.test_y.shape)\n",
        "    \n",
        "    def load_all_models(self):\n",
        "        all_models = list()\n",
        "        for i in range(self.n_members):\n",
        "            # define filename for this ensemble\n",
        "            filename = './drive/My Drive/ukdale'+str(self.eq-1)+'_model_' + str(i + 1) + '.h5'\n",
        "        # load model from file\n",
        "            model = load_model(filename)\n",
        "        # add to list of members\n",
        "            all_models.append(model)\n",
        "            print('>>> loaded %s' % filename)\n",
        "        return all_models\n",
        "\n",
        "    def stacked_dataset(self, inputX):\n",
        "        stackX = None\n",
        "        for model in self.members:\n",
        "            # make prediction\n",
        "            yhat = model.predict(inputX, verbose=0)\n",
        "            # stack predictions into [rows, members, probabilities]\n",
        "            if stackX is None:\n",
        "                stackX = yhat\n",
        "            else:\n",
        "                stackX = np.dstack((stackX, yhat))\n",
        "\n",
        "        stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
        "        return stackX\n",
        "\n",
        "    def fit_stacked_model(self, inputX, inputy, n_est=45, alpha = 10):\n",
        "      # create dataset using ensemble\n",
        "        stackedX = self.stacked_dataset(inputX)\n",
        "      # fit standalone model\n",
        "        self.ensemble = xgboost.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1, max_depth = 5, alpha = alpha, n_estimators = n_est)\n",
        "        self.ensemble.fit(stackedX, inputy)\n",
        "\n",
        "    def stacked_prediction(self, inputX):\n",
        "      # create dataset using ensemble\n",
        "        stackedX = self.stacked_dataset(inputX)\n",
        "      # make a prediction\n",
        "        yhat = self.ensemble.predict(stackedX)\n",
        "        return yhat\n",
        "\n",
        "    def make_cnnlstm_comparison_graph(self, n_test, idx=0):\n",
        "        preds = []\n",
        "        lstm_preds = []\n",
        "        for i in range(n_test):\n",
        "            lstm_preds.append(self.members[0].predict(self.test_x[i].reshape(1,-1,1))[0][0][0])\n",
        "        e_preds = self.stacked_prediction(self.test_x[idx:idx+n_test])\n",
        "        a_preds = self.load_arima_res()\n",
        "        preds.append(lstm_preds)            \n",
        "        preds.append(e_preds)\n",
        "        preds.append(a_preds[:n_test])\n",
        "        print(len(preds))\n",
        "        # np.save(file = './drive/My Drive/UkDale/preds_'+str(self.eq)+'.npy', arr = np.array(preds))\n",
        "\n",
        "        plt.plot(self.test_y[idx:idx+n_test].reshape(-1), c = 'orange', label='actual')\n",
        "        list_colrs = ['red', 'blue', 'red']\n",
        "        labels = ['CNN-LSTM', 'XG-Boost ensemble', 'ARIMA']\n",
        "        for j in range(3):\n",
        "            plt.plot(preds[j], c = list_colrs[j], label=labels[j])\n",
        "        plt.ylabel('Value')\n",
        "        plt.xlabel('Index of Time Series Data')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    \n",
        "    def arima_summary(x):\n",
        "        series = x[:, 0]\n",
        "        series = np.array(series)\n",
        "        model = ARIMA(series, order=(5,1,0))\n",
        "        model_fit = model.fit(disp=0)\n",
        "        print(model_fit.summary())\n",
        "        # plot residual errors\n",
        "        residuals = DataFrame(model_fit.resid)\n",
        "        residuals.plot()\n",
        "        pyplot.show()\n",
        "        residuals.plot(kind='kde')\n",
        "        pyplot.show()\n",
        "        print(residuals.describe())\n",
        "        \n",
        "    def arima_predictions():\n",
        "        x_test = self.test_x\n",
        "        y_test = self.test_y.reshape((-1,))\n",
        "        predictions = []\n",
        "        k = y_test.shape[0]\n",
        "        history = list(x_test[0].reshape(-1))\n",
        "        for t in range(k): \n",
        "            model = ARIMA(history, order=(5,1,0))\n",
        "            model_fit = model.fit(disp=0)\n",
        "            output = model_fit.forecast()\n",
        "            yhat = output[0]\n",
        "            predictions.append(yhat)\n",
        "            obs = test_y[t]\n",
        "            history.append(obs)\n",
        "            if(t%50==0):\n",
        "                print('predicted=%f, expected=%f' % (yhat, obs))\n",
        "\n",
        "        predictions = np.array(predictions)\n",
        "        return predictions\n",
        "    \n",
        "    def accuracy(test, pred):\n",
        "        test = test.reshape((-1,))\n",
        "        pred = pred.reshape((-1,))\n",
        "        error = mean_squared_error(test, pred)\n",
        "        print('Test MSE: %.3f' % error)\n",
        "        print(\"RMSE : %.3f\"%(np.sqrt(error)))\n",
        "        print(\"MAE : %.3f\"%(mean_absolute_error(test,pred)))\n",
        "        \n",
        "    def pipeline(self, n_test=100, idx=0, train=False, load=True):\n",
        "        self.load_data()\n",
        "        if(train):\n",
        "            # for i in range(self.n_members):\n",
        "            #     model = self.fit_cnn_lstm()\n",
        "            # filename = './drive/My Drive/ukdale' + str(self.eq-1) + '_model_' + str(i + 1) + '.h5'\n",
        "            # model.save(filename)\n",
        "            # print('>>> Saved %s' % filename)\n",
        "\n",
        "            self.ann = self.fit_ANN()\n",
        "            self.van_lstm = self.fit.fit_vanilla_lstm()\n",
        "          \n",
        "        if(load):\n",
        "            self.members = self.load_all_models()\n",
        "            print('Loaded %d models' % len(self.members))\n",
        "        self.fit_stacked_model(inputX = self.val_x, inputy = self.val_y)\n",
        "        self.make_cnnlstm_comparison_graph(n_test, idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wdKBc3px-PiM",
        "colab": {}
      },
      "source": [
        "d={}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M5_SfyOov35P",
        "colab": {}
      },
      "source": [
        "# d={}\n",
        "for i in range(15):\n",
        "    d[\"obj{0}\".format(i)] = Ensemble_pipeline(i+1, 3)\n",
        "    if(i != 3 and i != 5):\n",
        "        d[\"obj{0}\".format(i)].pipeline(n_test = 500, train = False)\n",
        "    elif(i == 5):\n",
        "        d[\"obj{0}\".format(i)].pipeline(n_test = 500, train = False, load = True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}